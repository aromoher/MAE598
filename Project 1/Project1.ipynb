{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Documentation of the problem formulation:\n",
    "\n",
    "For this formulation, I will be considering the initial positions and drag.\n",
    "\n",
    "Objective function:\n",
    "\n",
    "Variables:\n",
    "\n",
    "Constraints:\n",
    "\n",
    "Assumptions:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Programming:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# overhead\n",
    "\n",
    "import logging\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import time\n",
    "import torch as t\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "from torch.nn import utils\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# environment parameters\n",
    "\n",
    "FRAME_TIME = 0.01  # time interval\n",
    "GRAVITY_ACCEL = 9.81/1000  # gravity constant\n",
    "BOOST_ACCEL = 14.715/1000  # thrust constant, from class announcement\n",
    "\n",
    "#L_center_of_gravity = 5 / 1000\n",
    "\n",
    "# # the following parameters are not being used in the sample code\n",
    "# PLATFORM_WIDTH = 0.25  # landing platform width\n",
    "# PLATFORM_HEIGHT = 0.06  # landing platform height\n",
    "# ROTATION_ACCEL = 20  # rotation constant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define system dynamics\n",
    "# Notes: \n",
    "# 0. You only need to modify the \"forward\" function\n",
    "# 1. All variables in \"forward\" need to be PyTorch tensors.\n",
    "# 2. All math operations in \"forward\" has to be differentiable, e.g., default PyTorch functions.\n",
    "# 3. Do not use inplace operations, e.g., x += 1. Please see the following section for an example that does not work.\n",
    "\n",
    "class Dynamics(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Dynamics, self).__init__()\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(state, action):  \n",
    "\n",
    "        \"\"\"\n",
    "        action[0] = thrust controller \n",
    "        action[1] = delta theta controller (change in angle)\n",
    "        state[0] = x\n",
    "        state[1] = y\n",
    "        state[2] = x_dot (velocity in the x dir)\n",
    "        state[3] = y_dot (velocity in the y dir)\n",
    "        state[4] = theta\n",
    "        \"\"\"\n",
    "        \n",
    "        # Apply gravity\n",
    "        # gravity affects y_dot, so in a 5-by-1 matrix, it affects slot 4\n",
    "        delta_state_gravity = t.tensor([0., 0., 0., -GRAVITY_ACCEL * FRAME_TIME, 0.])\n",
    "        \n",
    "        # Apply theta\n",
    "        # Note: this is not delta_theta but rather the change from time step to time step\n",
    "        delta_state_theta = FRAME_TIME * t.mul([0., 0., 0., 0., 1.], action[:, 1].reshape(-1, 1)) #don't know if 1 or -1\n",
    "        #GO BACK ON THIS\n",
    "\n",
    "        # Thrust, action\n",
    "        # cos and sin are non linear, used example from slack but changed for my set up\n",
    "        N = len(state)\n",
    "        state_tensor = t.zeros((N, 5))\n",
    "        state_tensor[:, 2] = -t.sin(state[:, 4])\n",
    "        state_tensor[:, 3] = t.cos(state[:, 4])\n",
    "        delta_state = BOOST_ACCEL * FRAME_TIME * t.mul(state_tensor, action[:, 0].reshape(-1, 1))\n",
    "        \n",
    "\n",
    "        # Update velocity\n",
    "        state = state + delta_state + delta_state_gravity + delta_state_theta\n",
    "        \n",
    "        # Update state (for example, going from step 1 to step 2)\n",
    "        step_mat = t.tensor([[1., 0., FRAME_TIME, 0., 0.],\n",
    "                            [0., 1., 0., FRAME_TIME, 0.],\n",
    "                            [0., 0., 1., 0., 0.],\n",
    "                            [0., 0., 0., 1., 0.],\n",
    "                            [0., 0., 0., 0., 1.]])\n",
    "        state = t.matmul(step_mat, state)\n",
    "\n",
    "        return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate the inplace operation issue\n",
    "\n",
    "class Dynamics(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Dynamics, self).__init__()\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(state, action):\n",
    "\n",
    "        \"\"\"\n",
    "        action: thrust or no thrust\n",
    "        state[0] = y\n",
    "        state[1] = y_dot\n",
    "        \"\"\"\n",
    "\n",
    "        # Update velocity using element-wise operation. This leads to an error from PyTorch.\n",
    "        state[1] = state[1] + GRAVITY_ACCEL * FRAME_TIME - BOOST_ACCEL * FRAME_TIME * action\n",
    "        \n",
    "        # Update state\n",
    "        step_mat = t.tensor([[1., FRAME_TIME],\n",
    "                            [0., 1.]])\n",
    "        state = t.matmul(step_mat, state)\n",
    "\n",
    "        return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a deterministic controller\n",
    "# Note:\n",
    "# 0. You only need to change the network architecture in \"__init__\"\n",
    "# 1. nn.Sigmoid outputs values from 0 to 1, nn.Tanh from -1 to 1\n",
    "# 2. You have all the freedom to make the network wider (by increasing \"dim_hidden\") or deeper (by adding more lines to nn.Sequential)\n",
    "# 3. Always start with something simple\n",
    "\n",
    "class Controller(nn.Module):\n",
    "\n",
    "    def __init__(self, dim_input, dim_hidden, dim_output):    #customize\n",
    "        \"\"\"\n",
    "        dim_input: # of system states\n",
    "        dim_output: # of actions\n",
    "        dim_hidden: up to you\n",
    "        \"\"\"\n",
    "        super(Controller, self).__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(dim_input, dim_hidden),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(dim_hidden, dim_output),\n",
    "            # You can add more layers here\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, state):\n",
    "        action = self.network(state)\n",
    "        return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the simulator that rolls out x(1), x(2), ..., x(T)\n",
    "# Note:\n",
    "# 0. Need to change \"initialize_state\" to optimize the controller over a distribution of initial states\n",
    "# 1. self.action_trajectory and self.state_trajectory stores the action and state trajectories along time\n",
    "\n",
    "class Simulation(nn.Module):\n",
    "\n",
    "    def __init__(self, controller, dynamics, T):\n",
    "        super(Simulation, self).__init__()\n",
    "        self.state = self.initialize_state()\n",
    "        self.controller = controller\n",
    "        self.dynamics = dynamics\n",
    "        self.T = T\n",
    "        self.action_trajectory = []\n",
    "        self.state_trajectory = []\n",
    "\n",
    "    def forward(self, state):\n",
    "        self.action_trajectory = []\n",
    "        self.state_trajectory = []\n",
    "        for _ in range(T):\n",
    "            action = self.controller.forward(state)\n",
    "            state = self.dynamics.forward(state, action)\n",
    "            self.action_trajectory.append(action)\n",
    "            self.state_trajectory.append(state)\n",
    "        return self.error(state)\n",
    "\n",
    "    @staticmethod\n",
    "    def initialize_state():   #customize\n",
    "        state = [1., 0.]  # TODO: need batch of initial states\n",
    "        return t.tensor(state, requires_grad=False).float()\n",
    "\n",
    "    def error(self, state):\n",
    "        return state[0]**2 + state[1]**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up the optimizer\n",
    "# Note:\n",
    "# 0. LBFGS is a good choice if you don't have a large batch size (i.e., a lot of initial states to consider simultaneously)\n",
    "# 1. You can also try SGD and other momentum-based methods implemented in PyTorch\n",
    "# 2. You will need to customize \"visualize\"\n",
    "# 3. loss.backward is where the gradient is calculated (d_loss/d_variables)\n",
    "# 4. self.optimizer.step(closure) is where gradient descent is done\n",
    "\n",
    "class Optimize:\n",
    "    def __init__(self, simulation):\n",
    "        self.simulation = simulation\n",
    "        self.parameters = simulation.controller.parameters()\n",
    "        self.optimizer = optim.LBFGS(self.parameters, lr=0.01)\n",
    "\n",
    "    def step(self):\n",
    "        def closure():\n",
    "            loss = self.simulation(self.simulation.state)\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            return loss\n",
    "        self.optimizer.step(closure)\n",
    "        return closure()\n",
    "    \n",
    "    def train(self, epochs):\n",
    "        for epoch in range(epochs):\n",
    "            loss = self.step()\n",
    "            print('[%d] loss: %.3f' % (epoch + 1, loss))\n",
    "            self.visualize()\n",
    "\n",
    "    def visualize(self):       #customize\n",
    "        data = np.array([self.simulation.state_trajectory[i].detach().numpy() for i in range(self.simulation.T)])\n",
    "        x = data[:, 0]\n",
    "        y = data[:, 1]\n",
    "        plt.plot(x, y)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "one of the variables needed for gradient computation has been modified by an inplace operation: [torch.FloatTensor [1, 2]], which is output 0 of AsStridedBackward0, is at version 1; expected version 0 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\19282\\Desktop\\MAE598\\MAE598\\Project1.ipynb Cell 11\u001b[0m in \u001b[0;36m<cell line: 11>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/19282/Desktop/MAE598/MAE598/Project1.ipynb#X30sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m s \u001b[39m=\u001b[39m Simulation(c, d, T)  \u001b[39m# define simulation\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/19282/Desktop/MAE598/MAE598/Project1.ipynb#X30sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m o \u001b[39m=\u001b[39m Optimize(s)  \u001b[39m# define optimizer\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/19282/Desktop/MAE598/MAE598/Project1.ipynb#X30sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m o\u001b[39m.\u001b[39;49mtrain(\u001b[39m40\u001b[39;49m)\n",
      "\u001b[1;32mc:\\Users\\19282\\Desktop\\MAE598\\MAE598\\Project1.ipynb Cell 11\u001b[0m in \u001b[0;36mOptimize.train\u001b[1;34m(self, epochs)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/19282/Desktop/MAE598/MAE598/Project1.ipynb#X30sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtrain\u001b[39m(\u001b[39mself\u001b[39m, epochs):\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/19282/Desktop/MAE598/MAE598/Project1.ipynb#X30sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m     \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(epochs):\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/19282/Desktop/MAE598/MAE598/Project1.ipynb#X30sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m         loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstep()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/19282/Desktop/MAE598/MAE598/Project1.ipynb#X30sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m         \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39m[\u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m] loss: \u001b[39m\u001b[39m%.3f\u001b[39;00m\u001b[39m'\u001b[39m \u001b[39m%\u001b[39m (epoch \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m, loss))\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/19282/Desktop/MAE598/MAE598/Project1.ipynb#X30sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvisualize()\n",
      "\u001b[1;32mc:\\Users\\19282\\Desktop\\MAE598\\MAE598\\Project1.ipynb Cell 11\u001b[0m in \u001b[0;36mOptimize.step\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/19282/Desktop/MAE598/MAE598/Project1.ipynb#X30sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m     loss\u001b[39m.\u001b[39mbackward()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/19282/Desktop/MAE598/MAE598/Project1.ipynb#X30sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m loss\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/19282/Desktop/MAE598/MAE598/Project1.ipynb#X30sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptimizer\u001b[39m.\u001b[39;49mstep(closure)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/19282/Desktop/MAE598/MAE598/Project1.ipynb#X30sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m \u001b[39mreturn\u001b[39;00m closure()\n",
      "File \u001b[1;32mc:\\Users\\19282\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\optim\\optimizer.py:113\u001b[0m, in \u001b[0;36mOptimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    111\u001b[0m profile_name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mOptimizer.step#\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m.step\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(obj\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m)\n\u001b[0;32m    112\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mrecord_function(profile_name):\n\u001b[1;32m--> 113\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\19282\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\autograd\\grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[0;32m     25\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m     26\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclone():\n\u001b[1;32m---> 27\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\19282\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\optim\\lbfgs.py:311\u001b[0m, in \u001b[0;36mLBFGS.step\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    308\u001b[0m state\u001b[39m.\u001b[39msetdefault(\u001b[39m'\u001b[39m\u001b[39mn_iter\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m0\u001b[39m)\n\u001b[0;32m    310\u001b[0m \u001b[39m# evaluate initial f(x) and df/dx\u001b[39;00m\n\u001b[1;32m--> 311\u001b[0m orig_loss \u001b[39m=\u001b[39m closure()\n\u001b[0;32m    312\u001b[0m loss \u001b[39m=\u001b[39m \u001b[39mfloat\u001b[39m(orig_loss)\n\u001b[0;32m    313\u001b[0m current_evals \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\19282\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\autograd\\grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[0;32m     25\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m     26\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclone():\n\u001b[1;32m---> 27\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "\u001b[1;32mc:\\Users\\19282\\Desktop\\MAE598\\MAE598\\Project1.ipynb Cell 11\u001b[0m in \u001b[0;36mOptimize.step.<locals>.closure\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/19282/Desktop/MAE598/MAE598/Project1.ipynb#X30sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msimulation(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msimulation\u001b[39m.\u001b[39mstate)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/19282/Desktop/MAE598/MAE598/Project1.ipynb#X30sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/19282/Desktop/MAE598/MAE598/Project1.ipynb#X30sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/19282/Desktop/MAE598/MAE598/Project1.ipynb#X30sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m \u001b[39mreturn\u001b[39;00m loss\n",
      "File \u001b[1;32mc:\\Users\\19282\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\_tensor.py:396\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    387\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[0;32m    388\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    389\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[0;32m    390\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    394\u001b[0m         create_graph\u001b[39m=\u001b[39mcreate_graph,\n\u001b[0;32m    395\u001b[0m         inputs\u001b[39m=\u001b[39minputs)\n\u001b[1;32m--> 396\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs)\n",
      "File \u001b[1;32mc:\\Users\\19282\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\autograd\\__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    168\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[0;32m    170\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    171\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    172\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 173\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    174\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[0;32m    175\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: one of the variables needed for gradient computation has been modified by an inplace operation: [torch.FloatTensor [1, 2]], which is output 0 of AsStridedBackward0, is at version 1; expected version 0 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True)."
     ]
    }
   ],
   "source": [
    "# Now it's time to run the code!\n",
    "\n",
    "T = 100  # number of time steps\n",
    "dim_input = 2  # state space dimensions\n",
    "dim_hidden = 6  # latent dimensions\n",
    "dim_output = 1  # action space dimensions\n",
    "d = Dynamics()  # define dynamics\n",
    "c = Controller(dim_input, dim_hidden, dim_output)  # define controller\n",
    "s = Simulation(c, d, T)  # define simulation\n",
    "o = Optimize(s)  # define optimizer\n",
    "o.train(40)  # solve the optimization problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis of the results:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus:\n",
    "\n",
    "Idea: Projectile motion of ball. Throwing a ball to the same position from differing initial positions, using projectile motion equations."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b2d6e2acd905f85a7325b9d62875216e3aaf7a1996b68e1e27c6b306cb99bfde"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
